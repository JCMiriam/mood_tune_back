import pandas as pd
import torch
import numpy as np
import faiss  # 游댠 Importar FAISS para b칰squedas r치pidas
from sentence_transformers import SentenceTransformer

# 游늷 Ruta del archivo de embeddings generados con RoBERTa
EMBEDDINGS_FILE = r"C:\Users\solan\mood_tune_back\src\models\embeddings_roberta2.pkl"

# 游늷 Cargar el dataset con los embeddings
df = pd.read_pickle(EMBEDDINGS_FILE)

# 游늷 Convertir embeddings a NumPy arrays de tipo float32 (para FAISS)
df['embedding'] = df['embedding'].apply(lambda x: np.array(x, dtype=np.float32))

# 游늷 Crear un 칤ndice FAISS para b칰squeda r치pida (usamos IndexFlatIP para m치xima precisi칩n)
embeddings = np.stack(df["embedding"].values)  # Convertir embeddings a matriz
index = faiss.IndexFlatIP(embeddings.shape[1])  # 칈ndice con similitud coseno
index.add(embeddings)  # Cargar los embeddings en FAISS

# 游늷 Cargar modelo RoBERTa en GPU si est치 disponible
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer('all-roberta-large-v1', device=device)

# 游늷 Almacenar objetos en un diccionario para f치cil acceso desde otros archivos
model_data = {
    "df": df,
    "index": index,
    "model": model
}
